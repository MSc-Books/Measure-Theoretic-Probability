\section{Multiple Random Variables}

We begin by examining multiple random variables defined on a common probability space. Let's focus on two random variables, \(X\) and \(Y\), which are defined on the probability space \((\Omega, \mathcal{F}, P)\). It's crucial to grasp that the values taken by \(X\) and \(Y\) are influenced by the same underlying randomness, represented by \(\omega \in \Omega\).\\

For instance, consider a scenario involving weather on a specific day. We can let the random variable \(X\) represent the temperature of that day, while another random variable \(Y\) denotes the humidity level. Since both \(X\) and \(Y\) are determined by the same outcome, it is natural to expect a certain level of interdependence between them. In this weather example, a higher temperature generally correlates with increased humidity.\\

Say \(X\) and \(Y\) serve as measurable functions from the same probability space to the real numbers. The figure provided represents the pair \((X(\cdot), Y(\cdot))\) as a function mapping \(\Omega\) to \(\mathbb{R}^2\). This representation is particularly significant as it captures the interdependence between \(X\) and \(Y\).\\

A pertinent question arises: is the function \((X(\cdot), Y(\cdot)) : \Omega \to \mathbb{R}^2\) measurable, given that both \(X\) and \(Y\) are measurable functions? To properly address this question, we first need to define the Borel \(\sigma\)-algebra on \(\mathbb{R}^2\). The Borel \(\sigma\)-algebra on \(\mathbb{R}^2\), denoted as \(\mathcal{B}(\mathbb{R}^2)\), is generated by the collection 

\[
\mathcal{P}_{\mathbb{R}^2} = \{ (-\infty, x] \times (-\infty, y] \mid x, y \in \mathbb{R} \}.
\]

Thus, we can express this as 

\[
\mathcal{B}(\mathbb{R}^2) = \sigma(\mathcal{P}_{\mathbb{R}^2}).
\]


\begin{center}
\begin{tikzpicture}[
    >={Stealth[length=6pt]},
    probability space/.style={draw, minimum width=3cm, minimum height=4cm, align=center},
    real line/.style={draw, minimum width=2cm, minimum height=4cm},
    mapping/.style={->, thick, blue},
    point/.style={circle, fill, inner sep=1.5pt},
    label distance=0.5cm
]

% Define the probability space and real lines
\node[probability space] (omega) at (0,0) {};
\node[real line] (R2D) at (9,0) {$\mathbb{R}^2$};

% Add some sample points in probability space
\node[point, label=left:$\omega_1$] (w1) at (0,1) {};
\node[point, label=left:$\omega_2$] (w2) at (0,0) {};
\node[point, label=left:$\omega_3$] (w3) at (0,-1) {};

% Add points on real lines for Temperature and Humidity
\node[point] (x1) at (5,2.5) {};
\node[point] (x2) at (5,2) {};
\node[point] (x3) at (5,1.5) {};

\node[point] (y1) at (6,-1.5) {};
\node[point] (y2) at (6,-2) {};
\node[point] (y3) at (6,-2.5) {};

% Draw vertical lines for Temperature and Humidity
\draw[dashed] (5,-3) -> (5,3);
\draw[dashed] (6,-3) -> (6,3);

% Add points in R^2
\node[point] (xy1) at (8.5,0.5) {};
\node[point] (xy2) at (8.5,0) {};
\node[point] (xy3) at (8.5,-0.5) {};

% Draw mappings
\draw[mapping] (w1) to[bend left=20] node[above, midway] {$X$} (x1);
\draw[mapping] (w2) to[bend left=10] (x2);
\draw[mapping] (w3) to[bend left=0] (x3);

\draw[mapping] (w1) to[bend right=20] node[above, midway] {$Y$} (y1);
\draw[mapping] (w2) to[bend right=10] (y2);
\draw[mapping] (w3) to[bend right=0] (y3);

% Add the combined mapping
\draw[mapping, red] (w1) to[bend left=10]  (xy1);
\draw[mapping, red] (w2) to node[below, below] {$(X,Y)$} (xy2);
\draw[mapping, red] (w3) to[bend right=10] (xy3);

% Add labels and explanations
\node[align=center] at (0,-3) {Sample Space\\$(\Omega, \mathcal{F}, P)$};
\node[align=center] at (6,4) {Temperature\\Values\\$\in \mathcal{B}(\mathbb{R})$};
\node[align=center] at (5,-4) {Humidity\\Values\\$\in \mathcal{B}(\mathbb{R})$};
\node[align=center] at (9,-3) {Joint\\Distribution};
\end{tikzpicture}
\end{center}

\begin{theorem}
    Let \( X \) and \( Y \) be two random variables defined on the probability space \( (\Omega, \mathcal{F}, P) \). The mapping \( (X(\cdot), Y(\cdot)) : \Omega \to \mathbb{R}^2 \) is measurable with respect to \( \mathcal{F} \). This means that the pre-images of Borel sets in \( \mathbb{R}^2 \) under the mapping \( (X(\cdot), Y(\cdot)) \) correspond to events in the probability space.
\end{theorem}

\begin{proof}
    Let \( \mathcal{G} \) be the collection of all subsets of \( \mathbb{R}^2 \) such that their pre-images under \( (X(\cdot), Y(\cdot)) \) are events in \( \mathcal{F} \). To establish the theorem, it suffices to show that the Borel sets \( \mathcal{B}_{\mathbb{R}^2} \) are contained within \( \mathcal{G} \).\\

    The set \( \mathcal{G} \) is a \( \sigma \)-algebra of subsets of \( \mathbb{R}^2 \). This can be easily proved just by following through the definitions.\\

    1. \textbf{Non-emptiness:} \\
   We first show that \( \mathcal{G} \) is non-empty. The empty set \( \emptyset \) belongs to \( \mathcal{G} \) because its pre-image under any function, including \( (X(\cdot), Y(\cdot)) \), is the empty set, which is an event in \( \mathcal{F} \).\\

    2. \textbf{Closed under complements:}\\
    Let \( A \in \mathcal{G} \). By definition, the pre-image of \( A \) under \( (X(\cdot), Y(\cdot)) \) is an event in \( \mathcal{F} \). We denote this pre-image as \( (X,Y)^{-1}(A) \). The complement of \( A \), denoted \( A^c \), has the property that
    \[
    (X,Y)^{-1}(A^c) = \Omega \setminus (X,Y)^{-1}(A),
    \]
    which is also an event in \( \mathcal{F} \) since \( \mathcal{F} \) is a \( \sigma \)-algebra. Thus, \( A^c \in \mathcal{G} \).\\     

    3. \textbf{Closed under countable unions:}\\
    Let \( A_1, A_2, \ldots \) be a countable collection of sets in \( \mathcal{G} \). For each \( n \), the pre-image \( (X,Y)^{-1}(A_n) \) is an event in \( \mathcal{F} \). The pre-image of the union is given by
    \[
    (X,Y)^{-1}\left(\bigcup_{n=1}^{\infty} A_n\right) = \bigcup_{n=1}^{\infty} (X,Y)^{-1}(A_n),
    \]
    and since \( \mathcal{F} \) is a \( \sigma \)-algebra, this union is also an event in \( \mathcal{F} \). Hence, \( \bigcup_{n=1}^{\infty} A_n \in \mathcal{G} \).\\

    Since \( \mathcal{G} \) satisfies all three properties required for a \( \sigma \)-algebra, we conclude that \( \mathcal{G} \) is indeed a \( \sigma \)-algebra.\\

    Next, observe that the sets of the form \( \{ \omega \mid X(\omega) \leq x \} \) and \( \{ \omega \mid Y(\omega) \leq y \} \) belong to \( \mathcal{F} \) for all \( x, y \in \mathbb{R} \) because \( X \) and \( Y \) are random variables.\\

    Given that \( \mathcal{F} \) is a \( \sigma \)-algebra, the intersection of these two sets,
    \[
    \{ \omega \mid X(\omega) \leq x \} \cap \{ \omega \mid Y(\omega) \leq y \},
    \]
    also belongs to \( \mathcal{F} \) for all \( x, y \in \mathbb{R} \).\\

    Consequently, we conclude that 
    \[
    \{ \omega \mid X(\omega) \leq x, Y(\omega) \leq y \} \in \mathcal{F} \text{ for all } x, y \in \mathbb{R}.
    \]
    This implies that the rectangles \( (-\infty, x] \times (-\infty, y] \) belong to \( \mathcal{G} \) for every \( x, y \in \mathbb{R} \) based on the definition of \( \mathcal{G} \).\\

    From this, we can see that the collection of sets formed by all finite unions and complements of sets in \( \mathcal{G} \) will also belong to \( \mathcal{G} \), thus confirming that \( \mathcal{G} \) is indeed a \( \sigma \)-algebra.\\

    Finally, we observe that the collection of rectangles \( (-\infty, x] \times (-\infty, y] \) generates the Borel \( \sigma \)-algebra \( \sigma(\mathcal{P}(\mathbb{R}^2)) \), which contains all Borel sets in \( \mathbb{R}^2 \). Thus, we conclude that \( \mathcal{B}_{\mathbb{R}^2} \subseteq \mathcal{G} \).
\end{proof}

Let us denote the space of events as \(\mathcal{G}\), where the pre-images of Borel sets on \(\mathbb{R}^2\) correspond to events that we can assign probabilities to. This concept leads us to an important definition in probability, known as the \textit{joint probability law.}

\begin{definition}
    The joint probability law for the random variables \(X\) and \(Y\) is defined as follows:

\[
P_{X,Y}(B) = P(\{\omega \in \Omega \mid (X(\omega), Y(\omega)) \in B\}), \quad B \in \mathcal{B}(\mathbb{R}^2),
\]

where \(\mathcal{B}(\mathbb{R}^2)\) denotes the Borel \(\sigma\)-algebra on \(\mathbb{R}^2\). 
\end{definition}

This definition captures the idea that we are interested in the probability of the random vector \((X, Y)\) falling within the set \(B\). To illustrate this with a specific example, consider when we take the set \(B\) to be \((-\infty, x] \times (-\infty, y]\). In this case, the joint probability law can be expressed as:

\[
P_{X,Y}((-\infty, x] \times (-\infty, y]) = P(\{\omega \mid X(\omega) \leq x, Y(\omega) \leq y\}).
\]

Here, we are effectively computing the probability that the random variable \(X\) takes on a value less than or equal to \(x\) and simultaneously, the random variable \(Y\) takes on a value less than or equal to \(y\). This joint perspective allows us to understand the relationship between \(X\) and \(Y\) in a more comprehensive manner.

\subsection{Joint CDF and Its Properties}

\begin{definition}
    Let \( X \) and \( Y \) be two random variables defined on the probability space \( (\Omega, \mathcal{F}, P) \). The joint cumulative distribution function (CDF) of \( X \) and \( Y \) is defined as:

\[
F_{X,Y}(x,y) = P(\{\omega \mid X(\omega) \leq x, Y(\omega) \leq y\}), \quad \forall x,y \in \mathbb{R}.
\]
\end{definition}

In simpler terms, we denote this as \( F_{X,Y}(x,y) = P(X \leq x, Y \leq y) \).\\

\textbf{Properties of Joint CDF}\\

\begin{lemma}
    \textbf{Limits at Infinity.} We have the following limits:
    \[
    \lim_{x \to \infty, y \to \infty} F_{X,Y}(x,y) = 1, \quad \text{and} \quad \lim_{x \to -\infty, y \to -\infty} F_{X,Y}(x,y) = 0.
    \]
\end{lemma}

\begin{proof}
    Consider two unbounded, monotone-increasing sequences \( \{x_n\} \) and \( \{y_n\} \). Then we can express:
   \[
   \lim_{x \to \infty, y \to \infty} F_{X,Y}(x,y) = \lim_{x \to \infty, y \to \infty} P(X \leq x, Y \leq y) = \lim_{n \to \infty} P(X \leq x_n, Y \leq y_n).
   \]
   By using the properties of probability measures:
   \[
   = P\left(\bigcap_{n=1}^{\infty} \{\omega : X(\omega) \leq x_n, Y(\omega) \leq y_n\}\right) = P(\Omega) = 1.
   \]
   The proof for the second part follows a similar line of reasoning and is left as an exercise to the reader. Notably, the order in which we take the limits does not affect the result.
\end{proof}

\begin{lemma}
    \textbf{Monotonicity.} For any \( x_1 \leq x_2 \) and \( y_1 \leq y_2 \), it holds that:
    \[
    F_{X,Y}(x_1, y_1) \leq F_{X,Y}(x_2, y_2).
    \]
\end{lemma}

\begin{proof}
    Given \( x_1 \leq x_2 \) and \( y_1 \leq y_2 \), the event \( \{X \leq x_1, Y \leq y_1\} \) is a subset of the event \( \{X \leq x_2, Y \leq y_2\} \). Therefore, we can conclude:
   \[
   P(X \leq x_1, Y \leq y_1) \leq P(X \leq x_2, Y \leq y_2) \Rightarrow F_{X,Y}(x_1, y_1) \leq F_{X,Y}(x_2, y_2).
   \]
\end{proof}

\begin{lemma}
    \textbf{Continuity from Above.}  The joint CDF \( F_{X,Y} \) is continuous from above:
    \[
    \lim_{u \to 0^+, v \to 0^+} F_{X,Y}(x+u, y+v) = F_{X,Y}(x,y), \quad \forall x,y \in \mathbb{R}.
    \]
\end{lemma}

\begin{proof}
    To show that the joint CDF \( F_{X,Y} \) is continuous from above, we need to demonstrate that:

\[
\lim_{u \to 0^+, v \to 0^+} F_{X,Y}(x+u, y+v) = F_{X,Y}(x,y) \quad \forall x,y \in \mathbb{R}.
\]

consider the expression \( F_{X,Y}(x+u, y+v) \):

\[
F_{X,Y}(x+u, y+v) = P(X \leq x+u, Y \leq y+v).
\]

As \( u \to 0^+ \) and \( v \to 0^+ \), the events \( \{X \leq x+u\} \) and \( \{Y \leq y+v\} \) become increasingly close to the events \( \{X \leq x\} \) and \( \{Y \leq y\} \). \\

Specifically, we have:

\[
\{X \leq x\} \subseteq \{X \leq x+u\} \quad \text{and} \quad \{Y \leq y\} \subseteq \{Y \leq y+v\}.
\]

Therefore, we can express the probability:

\[
P(X \leq x+u, Y \leq y+v) \to P(X \leq x, Y \leq y) \quad \text{as } u \to 0^+ \text{ and } v \to 0^+.
\]

To formalize this, we can use the continuity of probability measures. For any \( \epsilon > 0 \), there exist sufficiently small \( u > 0 \) and \( v > 0 \) such that:

\[
\begin{aligned}
P(X \leq x+u, Y \leq y+v) &= P(X \leq x, Y \leq y) + P((X \leq x+u, Y \leq y+v) \setminus (X \leq x, Y \leq y)) \\
&\leq P(X \leq x, Y \leq y) + P(X > x, Y \leq y) + P(X \leq x, Y > y).
\end{aligned}
\]

As \( u \to 0^+ \) and \( v \to 0^+ \), both \( P(X > x, Y \leq y) \) and \( P(X \leq x, Y > y) \) converge to 0 due to the continuity of the probability measures. Thus, we conclude:

\[
\lim_{u \to 0^+, v \to 0^+} F_{X,Y}(x+u, y+v) = F_{X,Y}(x,y).
\]

\end{proof}

\begin{lemma}
    \textbf{Marginal CDFs.} We also have the property:
    \[
    \lim_{y \to \infty} F_{X,Y}(x,y) = F_X(x).
    \]
\end{lemma}

\begin{proof}
    Let \( \{y_n\} \) be an unbounded, monotone-increasing sequence. Then we can express:
   \[
   \lim_{y \to \infty} F_{X,Y}(x,y) = \lim_{n \to \infty} F_{X,Y}(x,y_n).
   \]
   This leads us to:
   \[
   = \lim_{n \to \infty} P(X \leq x, Y \leq y_n) = P\left(\bigcap_{n=1}^{\infty} \{\omega : X(\omega) \leq x, Y(\omega) \leq y_n\}\right) = P(\{\omega : X(\omega) \leq x\}) = F_X(x),
   \]
   where we again used the continuity of probability measures.
\end{proof}

