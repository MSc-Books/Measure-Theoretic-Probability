\section{Independence of Random Variables}

Before we proceed to define the independence of random variables, it is useful to understand the notion of the $\sigma$-algebra generated by a random variable.

\subsection{$\sigma$-algebra Generated by Random Variables}

We first state an elementary result that holds for any arbitrary function.

\begin{lemma}
    Let \(\Omega\) and \(S\) be two non-empty sets, and let \(f: \Omega \to S\) be a function. If \(H\) is a \(\sigma\)-algebra of subsets of \(S\), we aim to show that the collection 

\[
G = \{ A \mid A = f^{-1}(B), B \in H \}
\]

is a \(\sigma\)-algebra of subsets of \(\Omega\).
\end{lemma}

In words, the above lemma states that the collection of pre-images of all the sets belonging to some $\sigma$-algebra on the range of a function, is a $\sigma$-algebra on the domain of that function.\\

Let \((\Omega, \mathcal{F}, P)\) be a probability space, and let \(X: \Omega \to \mathbb{R}\) be a random variable. This random variable \(X\) induces a new probability triple \((\mathbb{R}, \mathcal{B}(\mathbb{R}), P_X)\) on the real line.\\

\begin{definition}
    We define the \(\sigma\)-algebra generated by the random variable \(X\) as follows:

\[
\mathcal{A}(X) = \{E \subseteq \Omega \mid E = X^{-1}(B), \, \forall B \in \mathcal{B}(\mathbb{R})\}.
\]
\end{definition}

This means that \(\mathcal{A}(X)\) consists of all events \(E\) in \(\Omega\) that can be obtained by taking the preimages of Borel sets under the mapping defined by the random variable \(X\).

\begin{lemma}
    \(\mathcal{A}(X) \subseteq \mathcal{F}\), which states that the \(\sigma\)-algebra generated by \(X\) is indeed a sub-\(\sigma\)-algebra of \(\mathcal{F}\).
\end{lemma}

To understand this better, note that each Borel set \(B\) corresponds to an event \(E\) in our probability space. The collection of all such preimages of Borel sets forms the \(\sigma\)-algebra generated by \(X\). Therefore, \(\mathcal{A}(X)\) includes exactly those events whose occurrence is fully determined by the value \(X(\omega)\) we observe.\\

Let's illustrate this concept with two examples:\\

\textbf{Example 1:} Consider a probability space \((\Omega, \mathcal{F}, P)\) and an event \(A \in \mathcal{F}\). Define the indicator random variable for the event \(A\) as \(I_A\). In this case, we can see that 

\[
\mathcal{A}(I_A) = \{\emptyset, A, A^c, \Omega\}.
\]

Thus, \(\mathcal{A}(I_A) \subseteq \mathcal{F}\).\\

\textbf{Example 2:} Let \(([0, 1], \mathcal{B}([0, 1]), \lambda)\) be our probability space, and consider the random variable \(X(\omega) = \omega\) for all \(\omega \in \Omega\). In this scenario, we find that 

\[
\mathcal{A}(X) = \mathcal{F}.
\]

From these examples, we observe that the \(\sigma\)-algebra generated by \(X\) can either be relatively small (as in Example 1) or as large as the entire \(\sigma\)-algebra \(\mathcal{F}\) itself (as in Example 2).

\subsection{Independence of Random Variables}

\begin{definition}
    Two random variables \(X\) and \(Y\) are independent if the \(\sigma\)-algebras generated by these variables, denoted as \(\sigma(X)\) and \(\sigma(Y)\), are independent as well. 
\end{definition}

To clarify, we can think of independence in terms of events defined by these random variables. Specifically, \(X\) and \(Y\) are independent if, for any two Borel sets \(B_1\) and \(B_2\) in \(\mathbb{R}\), the events corresponding to these sets can be expressed as follows:

\[
P\left(\{\omega : X(\omega) \in B_1\} \cap \{\omega : Y(\omega) \in B_2\}\right) = P\left(\{\omega : X(\omega) \in B_1\}\right) \cdot P\left(\{\omega : Y(\omega) \in B_2\}\right)
\]

for all \(B_1, B_2 \in \mathcal{B}(\mathbb{R})\). \\

This means that the probability of both \(X\) falling within \(B_1\) and \(Y\) falling within \(B_2\) simultaneously is simply the product of their individual probabilities of falling within these sets.\\

Additionally, there exists a helpful theorem that characterizes the independence of random variables in terms of their joint cumulative distribution function (CDF). 

\begin{theorem}
\(X\) and \(Y\) are independent if and only if the joint CDF of \(X\) and \(Y\) can be expressed as the product of their individual marginal CDFs.     
\end{theorem}

In mathematical terms, if \(F_{X,Y}(x,y)\) represents the joint CDF of \(X\) and \(Y\), and \(F_X(x)\) and \(F_Y(y)\) denote the marginal CDFs, then \(X\) and \(Y\) are independent if:

\[
F_{X,Y}(x,y) = F_X(x) \cdot F_Y(y)
\]

for all \(x, y\). This equivalence provides a practical method for checking the independence of random variables based on their distribution functions.

\begin{proof}
    To prove this theorem, we need to establish both the necessary and sufficient conditions for the independence of random variables \(X\) and \(Y\).\\

    \textbf{Proof of the necessary condition:} \\

Assume that \(X\) and \(Y\) are independent random variables. By the definition of independence, for any two Borel sets \(B_1 \in \mathcal{B}(\mathbb{R})\) and \(B_2 \in \mathcal{B}(\mathbb{R})\), the events \( \{ \omega \mid X(\omega) \in B_1 \} \) and \( \{ \omega \mid Y(\omega) \in B_2 \} \) are independent. This means that:

\[
P(X \in B_1, Y \in B_2) = P(X \in B_1) P(Y \in B_2).
\]

From the definition of the joint cumulative distribution function (CDF), we have:

\[
P(X \in B_1, Y \in B_2) = P_{X,Y}(B_1 \times B_2).
\]

Thus, we can write:

\[
P_{X,Y}(B_1 \times B_2) = P(X \in B_1) P(Y \in B_2).
\]

This holds for all Borel sets in \(\mathbb{R}\). \\

Now, we specifically choose the sets \(B_1 = (-\infty, x]\) and \(B_2 = (-\infty, y]\). Consequently, we obtain:

\[
F_{X,Y}(x, y) = P(X \leq x, Y \leq y) = P(X \leq x) P(Y \leq y) = F_X(x) F_Y(y),
\]

for all \(x, y \in \mathbb{R}\). This completes the proof of the necessary condition.\\

\textbf{Proof of the sufficient condition:}\\

Now, we will prove the sufficiency part. Assume that the joint cumulative distribution function (CDF) of \(X\) and \(Y\) can be expressed as:

\[
F_{X,Y}(x, y) = F_X(x) F_Y(y), \quad \forall x, y \in \mathbb{R}.
\]

To show that \(X\) and \(Y\) are independent, we need to demonstrate that for any Borel sets \(B_1\) and \(B_2\), the events defined by these sets are independent.\\

Using the property of joint CDFs, we can express the probability of the joint event as follows:

\[
P(X \in B_1, Y \in B_2) = P(X \leq x_1, Y \leq y_1) \quad \text{for } (x_1, y_1) \in B_1 \times B_2.
\]

Using the given expression for the joint CDF, we have:

\[
P(X \in B_1, Y \in B_2) = F_{X,Y}(x_1, y_1) = F_X(x_1) F_Y(y_1).
\]

Now, we also know:

\[
P(X \in B_1) = F_X(x_1) \quad \text{and} \quad P(Y \in B_2) = F_Y(y_1).
\]

Thus, we can write:

\[
P(X \in B_1, Y \in B_2) = P(X \in B_1) P(Y \in B_2).
\]

Since this holds for any arbitrary Borel sets \(B_1\) and \(B_2\), we conclude that the events \( \{X \in B_1\} \) and \( \{Y \in B_2\} \) are independent. \\

Therefore, we have established that if the joint CDF can be expressed as the product of the marginal CDFs, then \(X\) and \(Y\) are indeed independent.

\end{proof}

Let's extend our results to a general $n$ random variables. 

\begin{definition}    
Random variables \(X_1, X_2, \ldots, X_n\) are independent if the collections of events associated with these variables, represented by their \(\sigma\)-algebras \(\sigma(X_1), \sigma(X_2), \ldots, \sigma(X_n)\), are independent.
\end{definition}

This means that for any measurable sets \(B_i \in \mathcal{B}(\mathbb{R})\) for \(1 \leq i \leq n\), the joint probability can be expressed as the product of their individual probabilities. Mathematically, this is written as:

\[
P(X_1 \in B_1, X_2 \in B_2, \ldots, X_n \in B_n) = \prod_{i=1}^{n} P(X_i \in B_i).
\]

In simpler terms, knowing the outcome of one random variable does not give us any information about the others when they are independent.\\

Furthermore, we can characterize the independence of these random variables in terms of their cumulative distribution functions (CDFs). 

\begin{theorem}
    The random variables \(X_1, X_2, \ldots, X_n\) are independent if and only if their joint CDF can be expressed as the product of their individual CDFs. 

\[
F_{X_1, X_2, \ldots, X_n}(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} F_{X_i}(x_i).
\]
\end{theorem}

This theorem emphasizes that the joint distribution function of the independent random variables is completely determined by their individual distribution functions. In essence, this means that the behavior of the entire system of random variables can be understood by examining each variable independently, highlighting the fundamental nature of independence in probability.

\begin{definition}
A family of random variables \(\{X_i\}_{i \in I}\) as independent if the collections of events associated with these random variables, represented by their \(\sigma\)-algebras \(\{\sigma(X_i) : i \in I\}\), are independent. \\

This means that for any finite selection of indices \(i_1, i_2, \ldots, i_n\) from the index set \(I\), the joint probability of the corresponding random variables can be expressed as the product of their individual probabilities. Mathematically, for any sets \(B_{i_j} \in \mathcal{B}(\mathbb{R})\) for \(j = 1, 2, \ldots, n\), we have:

\[
P(X_{i_1} \in B_{i_1}, X_{i_2} \in B_{i_2}, \ldots, X_{i_n} \in B_{i_n}) = \prod_{j=1}^{n} P(X_{i_j} \in B_{i_j}).
\]
\end{definition}


In simpler words, the independence of the family of random variables implies that knowing the value of one variable provides no information about the values of the others. Thus, the concept of independence can be generalized beyond a finite number of random variables to an arbitrary family indexed by a set \(I\), reinforcing the idea that the individual behavior of each variable remains unaffected by the others in the family.